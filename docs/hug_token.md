# 🔑 Hugging Face アクセストークン発行 & 環境構築ガイド

ローカルLLM（`openai/gpt-oss-20b` 等）を動かすための認証設定手順です。

## 📋 目次
1. [Webでのトークン発行手順](#1-webでのトークン発行手順)
2. [【重要】Gated Modelの承認](#2-重要gated-modelの承認)
3. [環境変数への設定](#3-環境変数への設定)

---

## 1. Webでのトークン発行手順

Hugging Faceでローカルモデルをダウンロードするためには、Hugging Faceアカウントで「Read（読み取り）」権限を持つトークンを発行する必要があります。

1.  **設定ページへアクセス**
    Hugging Faceにログインし、右上のアイコンから **[Settings]** を選択します。
    

2.  **トークン管理画面を開く**
    左サイドバーの **[Access Tokens]** をクリックし、**[Create new token]** ボタンを押します。

3.  **権限の設定**
    以下の設定でトークンを作成します。
    * **Token type**: `Read` (モデルのダウンロードのみに使用するため)
    * **Name**: 任意の名前 (例: `local-gpu-server`)
    
    

4.  **トークンの保存**
    生成された `hf_` から始まる文字列をコピーし、安全な場所に保管してください。

> [!WARNING]
> **取り扱い注意**
> このトークンはパスワードと同じです。GitHubの公開リポジトリ等に誤ってアップロードしないよう厳重に管理してください。

---

## 2. 【重要】Gated Modelの承認

Llama 3やGemmaなど、一部のモデルはトークンがあるだけではダウンロードできません。モデルのページで利用規約に同意する必要があります。

* **確認方法**: 使いたいモデルのページ（例: `meta-llama/Meta-Llama-3-8B`）にアクセスします。
* **承認**: 画面上部に規約同意のフォームが表示されている場合は、必要事項を入力して **[Agree]** などをクリックしてください。
* **承認待ち**: ステータスが "Granted" になってから、次のステップへ進んでください。

> [!TIP]
> `openai/gpt-oss-20b` は通常オープンですが、新しいモデルを試す際は必ずこの「Gate認証」を確認する癖をつけてください。

---

## 3. 環境変数への設定

発行したトークンをLinux環境（vLLMを実行するサーバー）に設定します。

### 一時的な設定（推奨：テスト時）
現在のターミナルセッションのみ有効な設定です。

```bash
cd ~/Downloads
# 下記の hf_xxxxx を自分のトークンに書き換えて実行
export HUGGINGFACE_HUB_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```
